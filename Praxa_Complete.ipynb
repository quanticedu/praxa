{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanticedu/praxa/blob/complete-code/Praxa_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz1fotPlr1m3"
      },
      "source": [
        "#Lesson 3: Vector Database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Nc1lD3tYag"
      },
      "source": [
        "##Getting Started\n",
        "If you're new to Google Colab, download and review the [Getting Started with Colab](https://uploads.quantic.edu/assets/3330de11b37d7279540ad20e91425d070ced3ef92d7a5a296da55da40ca8d567/original/3330de11b37d7279540ad20e91425d070ced3ef92d7a5a296da55da40ca8d567.pdf) guide.\n",
        "\n",
        "Your code and data will run in the `/content` directory.\n",
        "\n",
        "The first task is to install libraries. Run the code in the following cell by clicking the play button on its left. Note that all commands at the shell prompt, such as `pip` below, should be preceded with a bang `!`. This will take a few minutes; while you're waiting you can get your OpenRouter API key (see the cell following this one for instructions).\n",
        "\n",
        "If Colab asks you to restart the run time at the end of the library installations, go ahead and do so. We recommend rerunning this first cell, although theoretically you shouldn't have to. If you get dependency errors, please notify Quantic at techsupport+msse@quantic.edu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokWSgM-zvDu"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.25 langchain-community==0.3.24 pypdf==5.5.0 langchain-text-splitters==0.3.8 sentence_transformers==4.1.0 langchain-chroma==0.2.4 langchain-huggingface==0.2.0 openai==1.82.0 streamlit==1.45.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll also need an API key from OpenRouter. Visit their [home page](https://openrouter.ai), select \"Sign in\" in the upper-right, then \"Sign up\" on the bottom of the dialog. The next dialog will have you create your account (we recommend using your Google account to sign in).\n",
        "\n",
        "Once you've created an account, go to the hamburger menu in the upper right and select \"Keys.\" Select \"Create API Key\" and follow the instructions.\n",
        "\n",
        "Once you have your API key, enter it below and run the code in the cell."
      ],
      "metadata": {
        "id": "6Z-gfaWF3BSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"<Your-API-Key>\""
      ],
      "metadata": {
        "id": "mjAaEQFVPJfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP7ReL0twnJ9"
      },
      "source": [
        "##Loading Context Documents\n",
        "The first step in building the vector database is to load the context documents. Load them into a variable named `context_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY4CoAHVz_h3"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "!mkdir context_data\n",
        "gdown.download(\"https://quanticedu.github.io/praxa/Longest Running Shows on Broadway 2025.pdf\", \"./context_data/Longest Running Shows on Broadway.pdf\", quiet=True)\n",
        "gdown.download(\"https://quanticedu.github.io/praxa/Every play and musical coming to the West End in 2025.pdf\", \"./context_data/Every play and musical coming to the West End in 2025.pdf\", quiet=True)\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"./context_data\")\n",
        "context_data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjgyps9f4uDg"
      },
      "source": [
        "Now let's verify that the documents loaded by printing the content of each page. Scroll to the end of a line to see what metadata the document loader includes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFtvWOhJ4y-w"
      },
      "outputs": [],
      "source": [
        "for page in context_data:\n",
        "  print(page)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RT2MMk6Jbj"
      },
      "source": [
        "##Chunking\n",
        "Now it's time to split the documents into chunks that will work with the LLM's context window. Store them in a variable named `chunks`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRUcG95A8jqB"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "chunks = text_splitter.split_documents(context_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofngPyLR_EXV"
      },
      "source": [
        "Verify it worked by exploring how the documents were chunked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx_t2f8U-k9d"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Document Chunks: {len(chunks)}\")\n",
        "\n",
        "for num, chunk in enumerate(chunks):\n",
        "  print(\"------\")\n",
        "  print(f\"Chunk {num}:\")\n",
        "  print(f\"Length: {len(chunk.page_content)}\")\n",
        "  print(f\"Metadata: {chunk.metadata}\")\n",
        "  print(f\"Content: {chunk.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnR0lbf-CSm1"
      },
      "source": [
        "##Embedding\n",
        "\n",
        "Now it's time to set up the embedding function. Assign it to a variable named `embedding_function`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln3cVD-XCWIg"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYCpOZU9DvI0"
      },
      "source": [
        "Make sure your model works by finding the embedding for a test sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RaQ_S0eDzix"
      },
      "outputs": [],
      "source": [
        "embedding = embedding_function.embed_query(\"This is a test sentence.\")\n",
        "print(f\"Embedding length: {len(embedding)}\")\n",
        "embedding = embedding_function.embed_query(\"This is a longer test sentence.\")\n",
        "print(f\"Embedding length: {len(embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGeqzZA8IQqv"
      },
      "source": [
        "##Persisting\n",
        "\n",
        "Now it's time for the vector store. Assign it the name `chromadb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-KT3aALInxP"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "chromadb = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=\"./chromadb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pmh1FmFLIE3"
      },
      "source": [
        "Now test it by executing a similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uwCgpxrLO1_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "retrieved_chunks = chromadb.similarity_search(\"A play written by Ryan Calais Cameron.\")\n",
        "print(f\"Query retrieved {len(retrieved_chunks)} chunks.\")\n",
        "for chunk in retrieved_chunks:\n",
        "  print(f\"Chunk content: {chunk.page_content}\")\n",
        "  print(f\"Chunk metadata: {chunk.metadata}\")\n",
        "  print(\"-----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b4gTUZ6yPAP"
      },
      "source": [
        "#Lesson 4: LangChain and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLsPvc7t6Pqn"
      },
      "source": [
        "###Getting the LLM\n",
        "OpenRouter offers free access to many LLMs: https://openrouter.ai/models?max_price=0\n",
        "\n",
        "It is important to note that the free models have low rate limits (50 requests per day total), which is great for prototyping but usually not suitable for production use.\n",
        "\n",
        "Since OpenRouter provides an API that is compatible with OpenAI, you can seamlessly integrate it into your LLM application by using the ChatOpenAI class from LangChain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from typing import Optional\n",
        "\n",
        "class ChatOpenRouter(ChatOpenAI):\n",
        "    openai_api_base: str\n",
        "    openai_api_key: str\n",
        "    model_name: str\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name: str,\n",
        "                 openai_api_key: Optional[str] = None,\n",
        "                 openai_api_base: str = \"https://openrouter.ai/api/v1\",\n",
        "                 **kwargs):\n",
        "        openai_api_key = openai_api_key or os.getenv('OPENROUTER_API_KEY')\n",
        "        super().__init__(openai_api_base=openai_api_base,\n",
        "                         openai_api_key=openai_api_key,\n",
        "                         model_name=model_name, **kwargs)\n",
        "\n",
        "llm = ChatOpenRouter(\n",
        "    model_name=\"google/gemma-3-27b-it:free\",\n",
        "    max_tokens=512,\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "nXN0O46QvTmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ9gZIxR__Oq"
      },
      "source": [
        "Let's invoke the LLM with a few prompts it should be able to handle. Take note of the answers, which are based solely on the model's training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xagZYN4GAQA7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "response = llm.invoke(\n",
        "    [SystemMessage(\"You are a helpful assistant.\"),\n",
        "     HumanMessage(\"What are some plays by Tawfiq al-Hakim?\")])\n",
        "print(response.content)\n",
        "print(\"----------\")\n",
        "response = llm.invoke(\n",
        "    [SystemMessage(\"You are a helpful assistant.\"),\n",
        "     HumanMessage(\"What is Ryan Calais Camerons's most recent play?\")])\n",
        "print(response.content)\n",
        "print(\"----------\")\n",
        "response = llm.invoke(\n",
        "    [SystemMessage(\"You are a helpful assistant.\"),\n",
        "     HumanMessage(\"What Broadway shows have more than 10,000 performances?\")])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwwSFj5Dr5L"
      },
      "source": [
        "###Setting up a Chat Prompt Template\n",
        "We'll now build a simple prompt template to make our interface with the LLM a bit more generic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2gdIykGEBNQ"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"user\", \"What is {playwright}'s most recent play?\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kXZPOpcEkvH"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_template.invoke({\"playwright\": \"Ryan Calais Cameron\"}))\n",
        "\n",
        "response = llm.invoke(prompt_template.invoke({\"playwright\": \"Ryan Calais Cameron\"}))\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "0shNAKurRiLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYJ5TPaqPiD8"
      },
      "source": [
        "## LangChain Expression Language (LCEL)\n",
        "The \"Chain\" in \"LangChain\" refers to the ability to chain several actions into one invocation. This replaces your nested calls to `llm.invoke()`, and `chat_prompt.invoke()`. Try to build a chain for what you have here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj-_rm2dQNr1"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm\n",
        "response = chain.invoke(\n",
        "    {\"playwright\" : \"Ryan Calais Cameron\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3VlXuSfB2_5"
      },
      "source": [
        "#Lesson 5: RAG Using LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrMBqouGB_5U"
      },
      "source": [
        "##Build a Prompt Template\n",
        "We'll start with a prompt template that combines the context and original question and provides instructions to the model on how to use both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6LBTX5NCTo7"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"\"\"You are an assistant\n",
        "        providing answers to questions\n",
        "        about the theater. In addition to\n",
        "        your training data, you are to\n",
        "        use the additional context\n",
        "        provided below to provide\n",
        "        up-to-date information.\"\"\"),\n",
        "    (\"user\", \"\"\"Question:\n",
        "        {question}\\nContext:\n",
        "        {context}\"\"\")])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vSm7m7FJVt"
      },
      "source": [
        "To get the context, we'll use a *retriever*. It takes a string as the input query and returns a `list` of `Document` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEEdbwfMFqCv"
      },
      "outputs": [],
      "source": [
        "retriever = chromadb.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgeUSMPF3gs"
      },
      "source": [
        "Run it to see what it outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TZ0fq9UF6CN"
      },
      "outputs": [],
      "source": [
        "docs = retriever.invoke(\"What is Ryan Calais Cameron's most recent play?\")\n",
        "print(f\"Found {len(docs)} documents:\")\n",
        "\n",
        "for doc in docs:\n",
        "  print(\"------\")\n",
        "  print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgki18WqiXZj"
      },
      "source": [
        "The final form we're going for is `chain.invoke(user_question)`. We'll need the `user_question` for two things in this prompt: the question itself and finding the context from the vector database. Doing multiple things to one input is the job of a `RunnableParallel`. Let's create one that does that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meXqYUTfieuY"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
        "question_and_docs = RunnableParallel(\n",
        "    { \"question\": RunnablePassthrough(),\n",
        "      \"context_docs\": retriever }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQnER6Cis8Z"
      },
      "source": [
        "Let's see what that looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8gW0YKlivfC"
      },
      "outputs": [],
      "source": [
        "question_and_docs.invoke(\"What is Ryan Calais Cameron's most recent play?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RunnablePassthrough` has a static method named `assign()`, which adds keys to a dictionary by applying a function to that dictionary. Here's a simple example."
      ],
      "metadata": {
        "id": "2JGKhJNqDhim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {\n",
        "    \"question\": \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "    \"answer\": \"All the wood that a woodchuck could chuck if a woodchuck could chuck wood.\"\n",
        "}\n",
        "\n",
        "add_length = RunnablePassthrough.assign(length=len)\n",
        "print(type(add_length))\n",
        "add_length.invoke(my_dict)"
      ],
      "metadata": {
        "id": "oWx9jaKmDwsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr3BOS2tJvch"
      },
      "source": [
        "To use the context docs in a prompt, we're going to need to convert them to a string. We'll use a `RunnablePassthrough` to assign that string to the `context` key the prompt needs. Note that the `question` attribute from `context_docs_and_question` gets passed through."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuTlQxcWJvIh"
      },
      "outputs": [],
      "source": [
        "def make_context_string(dict_with_docs):\n",
        "    # Take the page_content attribute of each Document object\n",
        "    # and join them into one string, separated by two newlines.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in dict_with_docs[\"context_docs\"])\n",
        "\n",
        "context = RunnablePassthrough.assign(context=make_context_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfjcNldLqlF"
      },
      "source": [
        "Let's see how all this works with our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcTaUM1aLt-a"
      },
      "outputs": [],
      "source": [
        "complete_prompt_chain = question_and_docs | context | prompt_template\n",
        "complete_prompt_chain.invoke(\"What is Ryan Calais Cameron's most recent play?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDzUuMVyMLF1"
      },
      "source": [
        "Now we'll build the final chain for our app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAiLsZ0gMPnT"
      },
      "outputs": [],
      "source": [
        "chain = question_and_docs | context | prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tx95TvIMW5l"
      },
      "source": [
        "And run it to see what results we get. Here we should see that \"Retrograde\" is the response, even though it occurred after the training cutoff of the model.\n",
        "\n",
        "If you don't see \"Retrograde\", try starting a new Colab runtime and running the code in the notebook again. This resets the model and clears cached responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwaDZ1jZMaBJ"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke(\"What is Ryan Calais Cameron's most recent play?\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAIHm-WfIbp"
      },
      "source": [
        "Now we'll build a chain that passes the source citations, which were in the metadata field of the `list` of `Document` objects returned from the retriever. We'll use `RunnableParallel` to pass the `list` to the end of the chain while also passing it to a chain that builds the prompt and invokes the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdhhFAO4f-Uo"
      },
      "outputs": [],
      "source": [
        "answer_chain = (context\n",
        "    | prompt_template\n",
        "    | llm)\n",
        "chain_with_sources = \\\n",
        "    question_and_docs.assign(\n",
        "        answer=answer_chain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCAcF-Y3glRJ"
      },
      "source": [
        "Now run it to see what we got. When we asked this question without context, the model listed dated figures (e.g., 10,737 shows for \"Chicago\"). The model read the context data and formed a response that answered the question directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc8HoXULp88X"
      },
      "outputs": [],
      "source": [
        "result = chain_with_sources.invoke(\"What Broadway shows have more than 10,000 performances?\")\n",
        "print(\"The docs used in this answer:\")\n",
        "print(\"\\n\".join(doc.metadata.__repr__() for doc in result[\"context_docs\"]))\n",
        "print(\"----------------------------\")\n",
        "print(\"\\nThe answer:\")\n",
        "print(result[\"answer\"].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfdS__hkl8g3"
      },
      "source": [
        "#Lesson 6: User Interface\n",
        "While not directly related to LLMs or AI in general, user interfaces are essential to making an app approachable. We'll use Streamlit to build a basic front end for our app.\n",
        "##Getting Started\n",
        "First we need to install an npm package that will allow us to expose the Colab runtime to IP traffic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_HnWFCYnwjD"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cGFC8MndmL"
      },
      "source": [
        "Now we'll create a simple \"Hello World\" app. This illustrates how simple Streamlit is to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMauEgLqoHts"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.write(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q8ximhRoPW0"
      },
      "source": [
        "Now we need to run the app and view it in a browser. There are a few steps to this:\n",
        "* Start the Streamlit server using the *app.py* script.\n",
        "* Set up a local tunnel to get a URL that connects to the Colab runtime.\n",
        "* Get the public IP of the Colab runtime to gain access to the localtunnel-created URL.\n",
        "\n",
        "We'll do this all on one command line. The command will display the public IP of the Colab runtime then a link to the Streamlit server. When you click the link you'll be asked for the tunnel password, which is the Colab runtime's public IP.\n",
        "\n",
        "Note that this cell will continue running the server until you manually stop it. No other cells in the notebook can run while this cell is running. Stop the cell by selecting the stop button to its left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qjxpn_pP_U"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbw1ZpjqnWV"
      },
      "source": [
        "##Building the Backend\n",
        "Up to this point we've been running the Python instructions in interactive mode in the Colab notebook. For our app to work as a backend, we need to make the code available in a module that the front end can import. Let's go back through our code and copy the needed elements into a single Python file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7inJlGkWrNkh"
      },
      "outputs": [],
      "source": [
        "%%writefile backend.py\n",
        "import os\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from typing import Optional\n",
        "\n",
        "# 1. Set ChatPromptTemplate for RAG\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant providing answers to questions about the theater. \"\n",
        "               \"In addition to your training data, use the additional context provided below to provide up-to-date information.\"),\n",
        "    (\"user\", \"Question: {question}\\nContext: {context}\\nAnswer:\")\n",
        "])\n",
        "\n",
        "# 2. Context Retriever\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma(persist_directory=\"./chromadb\",\n",
        "                  embedding_function=embedding_function)\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "#3. LCEL Chain Setup\n",
        "question_and_docs = RunnableParallel(\n",
        "    {\"context_docs\": retriever, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "def make_context_string(to_convert):\n",
        "    # Take the page_content attribute of each Document object\n",
        "    # and join them into one string, separated by two newlines.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in to_convert[\"context_docs\"])\n",
        "\n",
        "context = RunnablePassthrough.assign(context=make_context_string)\n",
        "\n",
        "# 4. LLM\n",
        "class ChatOpenRouter(ChatOpenAI):\n",
        "    openai_api_base: str\n",
        "    openai_api_key: str\n",
        "    model_name: str\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_name: str,\n",
        "                 openai_api_key: Optional[str] = None,\n",
        "                 openai_api_base: str = \"https://openrouter.ai/api/v1\",\n",
        "                 **kwargs):\n",
        "        openai_api_key = openai_api_key or os.getenv('OPENROUTER_API_KEY')\n",
        "        super().__init__(openai_api_base=openai_api_base,\n",
        "                         openai_api_key=openai_api_key,\n",
        "                         model_name=model_name, **kwargs)\n",
        "\n",
        "llm = ChatOpenRouter(\n",
        "    model_name=\"google/gemma-3-27b-it:free\",\n",
        "    max_tokens=512,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# 5. Build answering Chain\n",
        "answer_chain = context | prompt | llm\n",
        "\n",
        "chain_with_sources = question_and_docs.assign(answer=answer_chain)\n",
        "\n",
        "\n",
        "# 6. Final method to invoke the chain and get answer and sources\n",
        "def answer_and_sources(question):\n",
        "    result = chain_with_sources.invoke(question)\n",
        "    response_text = result[\"answer\"].content\n",
        "    #answer_index = response_text.rfind(\"Answer:\")\n",
        "    #answer_text = response_text[answer_index + len(\"Answer:\"):].strip()\n",
        "    sources = \"\\n\\n\".join(f\"{doc.metadata['source']}, page {doc.metadata['page']}\" for doc in result[\"context_docs\"])\n",
        "    #return {\"answer\": answer_text,\n",
        "    return {\"answer\": response_text,\n",
        "            \"sources\": sources\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NshhzFpfycmu"
      },
      "source": [
        "Now test the back end manually to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_XXDxW8ygky"
      },
      "outputs": [],
      "source": [
        "import backend, importlib\n",
        "importlib.reload(backend)\n",
        "print(backend.answer_and_sources(\"What is Ryan Calais Cameron's most recent play?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgBSvp4T21Rf"
      },
      "source": [
        "##Building the Interface\n",
        "Now let's use Streamlit's example chat app to build the interface for Praxa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCUV65PErwP9"
      },
      "outputs": [],
      "source": [
        "%%writefile praxa.py\n",
        "import streamlit as st\n",
        "import importlib, backend\n",
        "importlib.reload(backend)\n",
        "\n",
        "st.title(\"Praxa\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if question := st.chat_input(\"Ask me about the theater!\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(question)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "    response = backend.answer_and_sources(question)\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response['answer'])\n",
        "        st.markdown(response['sources'])\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response['answer']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu-P0k8g3lpl"
      },
      "source": [
        "Now we run the whole app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX4RcudXAo4N"
      },
      "outputs": [],
      "source": [
        "!streamlit run praxa.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}